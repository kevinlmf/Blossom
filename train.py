"""
Multi-Frequency Trading System with Market Regime Classification

Architecture:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         üéØ Market Regime Detector (Top Level)               ‚îÇ
‚îÇ  Input: Historical price data                               ‚îÇ
‚îÇ  Output: Regime classification & adapted parameters         ‚îÇ
‚îÇ         [HIGH_RISK | HIGH_RETURN | STABLE]                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ (auto-adjust parameters)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Allocator Agent (Meta Level - PPO)                  ‚îÇ
‚îÇ  Input: Agent performance, latent factors, macro indicators ‚îÇ
‚îÇ  Output: Capital allocation [œÄ_HFT, œÄ_MFT, œÄ_LFT]          ‚îÇ
‚îÇ  Objective: max E[R] - Œª¬∑CVaR(R_total)                     ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                        ‚îÇ
‚îÇ         ‚îÇ üß† Portfolio Memory Bank ‚îÇ                        ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ              ‚îÇ              ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ HFT Agent ‚îÇ  ‚îÇ MFT Agent ‚îÇ  ‚îÇ LFT Agent ‚îÇ
‚îÇ   (SAC)   ‚îÇ  ‚îÇ   (SAC)   ‚îÇ  ‚îÇ   (SAC)   ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇüß† CBR ‚îÇ ‚îÇ  ‚îÇ ‚îÇüß† CBR ‚îÇ ‚îÇ  ‚îÇ ‚îÇüß† CBR ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ Tick-level‚îÇ  ‚îÇHour/Daily ‚îÇ  ‚îÇPortfolio  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ              ‚îÇ              ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ  Shared Encoder (Transformer)‚îÇ
      ‚îÇ  Latent Factors: z_t         ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ   Risk Controller            ‚îÇ
      ‚îÇ   - CVaR monitoring          ‚îÇ
      ‚îÇ   - Max Drawdown control     ‚îÇ
      ‚îÇ   - Position adjustment      ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ   Hedge Manager (ÊúÄÂêé‰∏ÄÊ≠•)   ‚îÇ
      ‚îÇ   - Excess ‚Üí Absolute return ‚îÇ
      ‚îÇ   - Market neutral strategy  ‚îÇ
      ‚îÇ   - Beta hedging            ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Usage:
    # Auto-detect regime and train
    python train.py --data-path data.csv

    # Train on specific regime
    python train.py --regime high_risk --episodes 1000

    # Train on all regimes
    python train.py --mode all_regimes --episodes 500
"""

import os
import sys
import argparse
import yaml
from pathlib import Path
from typing import Dict, Any, Optional
import json

# Parse GPU flag before importing JAX
_requested_gpu = "--gpu" in sys.argv
if not _requested_gpu:
    os.environ['JAX_PLATFORM_NAME'] = 'cpu'

import jax
import numpy as np
import pandas as pd

# Import experiments framework
from experiments import RealDataLoader, MarketRegimeDetector, StrategyMonitor
from agent.memory import StrategyMemoryBank
from agent.offline_pretrainer import OfflinePretrainer

# Goal-conditioned imports
from goal import GoalDefinition, GoalParser, GoalStateTracker
from agent.planner import GoalPlanner, PlannerInputs

# Import evaluation framework
from evaluation import GoalAudit, StrategyEvaluator

# Import encoder factory for regime-adaptive architecture
from shared_encoder import create_encoder, get_encoder_info

print(f"JAX devices: {jax.devices()}")
print(f"JAX version: {jax.__version__}")


class RegimeAdaptiveTrainingSystem:
    """
    Complete training system with market regime classification at the top level.

    ÂÆåÊï¥ÊµÅÁ®ãÔºö
    1. ËæìÂÖ•ÂéÜÂè≤Êï∞ÊçÆ ‚Üí Â∏ÇÂú∫Âë®ÊúüÂàÜÁ±ª
    2. ÂàùÂßãÂåñÁ≥ªÁªüÊû∂ÊûÑÔºàAllocator + HFT/MFT/LFT agentsÔºâ
    3. ‰ªéËÆ∞ÂøÜÂ∫ìÊ£ÄÁ¥¢ÊúÄ‰ºòÁ≠ñÁï•ÔºàWarm StartÔºâ
    4. RLËÆ≠ÁªÉÔºàÂú®warm startÂü∫Á°Ä‰∏ä‰ºòÂåñÔºâ
    5. Êõ¥Êñ∞Á≠ñÁï•ËÆ∞ÂøÜÂ∫ì
    """

    def __init__(
        self,
        goal_definition: GoalDefinition,
        output_dir: str = "outputs",
        memory_dir: str = "memory_bank",
        verbose: bool = True
    ):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.verbose = verbose
        self.goal_definition = goal_definition

        # Initialize top-level components
        self.regime_detector = MarketRegimeDetector()
        self.data_loader = RealDataLoader(verbose=verbose)

        # Initialize Strategy Memory Bank (CBR for all agents)
        self.memory_bank = StrategyMemoryBank(memory_dir=memory_dir)
        self.offline_pretrainer = OfflinePretrainer(self.memory_bank, top_k=3)

        # Initialize Strategy Evaluator
        self.evaluator = StrategyEvaluator(
            output_dir=f"{output_dir}/evaluation",
            risk_free_rate=0.02,
            periods_per_year=252
        )
        self.goal_tracker = GoalStateTracker(goal_definition)
        self.goal_planner = GoalPlanner(goal_definition)
        self.goal_audit = GoalAudit(output_dir=f"{output_dir}/goal_audit")

        if verbose:
            print("\n" + "=" * 80)
            print("üéØ REGIME-ADAPTIVE MULTI-FREQUENCY TRADING SYSTEM WITH CBR")
            print("=" * 80)
            print("\nArchitecture:")
            print("  ‚îå‚îÄ Market Regime Detector")
            print("  ‚îú‚îÄ Strategy Memory Bank (CBR)")
            print("  ‚îÇ  ‚îú‚îÄ HFT Memory")
            print("  ‚îÇ  ‚îú‚îÄ MFT Memory")
            print("  ‚îÇ  ‚îú‚îÄ LFT Memory")
            print("  ‚îÇ  ‚îî‚îÄ Allocator Memory")
            print("  ‚îú‚îÄ Allocator Agent (PPO)")
            print("  ‚îú‚îÄ HFT Agent (SAC)")
            print("  ‚îú‚îÄ MFT Agent (SAC)")
            print("  ‚îú‚îÄ LFT Agent (SAC)")
            print("  ‚îú‚îÄ Shared Encoder")
            print("  ‚îî‚îÄ Risk Controller (DCC)")
            print("=" * 80)
            print("\nGoal Definition Loaded:")
            print(f"  Name: {goal_definition.name}")
            print(f"  Owner: {goal_definition.owner}")
            print(f"  Description: {goal_definition.description}")
            weights = goal_definition.normalized_weights
            print(f"  Utility Weights: {weights}")
            if goal_definition.constraints.max_drawdown is not None:
                print(f"  Max Drawdown Constraint: {goal_definition.constraints.max_drawdown}")
            if goal_definition.constraints.latency_limit_ms is not None:
                print(f"  Latency Budget (ms): {goal_definition.constraints.latency_limit_ms}")

    def train_with_regime_detection(
        self,
        data_source: str = "yahoo",
        symbols: list = None,
        start_date: str = None,
        end_date: str = None,
        num_episodes: int = 500,
        steps_per_episode: int = 100,
        seed: int = 42
    ) -> Dict[str, Any]:
        """
        Full pipeline: Detect regime ‚Üí Adapt parameters ‚Üí Train system

        Args:
            data_source: Data source ('yahoo', 'csv', 'predefined')
            symbols: List of symbols to trade
            start_date: Start date
            end_date: End date
            num_episodes: Training episodes
            steps_per_episode: Steps per episode
            seed: Random seed

        Returns:
            Training results with regime information
        """
        print("\n" + "=" * 80)
        print("STEP 1/4: LOADING DATA")
        print("=" * 80)

        # Load market data
        if data_source == "predefined":
            return self.train_on_predefined_regimes(num_episodes, steps_per_episode, seed)

        market_data = self._load_data(data_source, symbols, start_date, end_date)

        print("\n" + "=" * 80)
        print("STEP 2/4: MARKET REGIME CLASSIFICATION")
        print("=" * 80)

        # Detect market regime
        main_symbol = list(market_data.keys())[0]
        prices = market_data[main_symbol]['Close'].values
        dates = market_data[main_symbol].index.to_pydatetime().tolist()

        detected_periods = self.regime_detector.detect_specific_periods(
            prices=prices,
            dates=dates,
            min_period_length=20
        )

        # Print detected regimes
        print(f"\nüìä Market Regime Analysis:")
        for regime_type, periods in detected_periods.items():
            if periods:
                print(f"  {regime_type.upper()}: {len(periods)} periods detected")
                # Show the most prominent period
                longest = max(periods, key=lambda p: p.end_idx - p.start_idx)
                print(f"    ‚Üí Longest: {longest.end_idx - longest.start_idx} days")
                print(f"       Volatility: {longest.volatility:.4f}")
                print(f"       Avg Return: {longest.avg_return:.4f}")

        # Determine primary regime
        primary_regime = self._determine_primary_regime(detected_periods)

        print(f"\nüéØ PRIMARY REGIME: {primary_regime.upper()}")
        print(f"   ‚Üí Adapting system parameters for {primary_regime} market conditions")

        print("\n" + "=" * 80)
        print("STEP 3/4: CONFIGURING REGIME-ADAPTIVE PARAMETERS")
        print("=" * 80)

        # Get regime-specific configuration
        config = self._get_regime_adaptive_config(primary_regime, market_data)

        print("\nüìã Configuration:")
        print(f"  Risk Aversion: {config['lft_risk_aversion']}")
        print(f"  Transaction Cost: {config['hft_transaction_cost']}")
        print(f"  Min Allocation: {config['min_allocation_per_agent']}")
        print(f"  Learning Rate: {config['allocator_lr']}")

        print("\n" + "=" * 80)
        print("STEP 4/4: TRAINING MULTI-FREQUENCY SYSTEM")
        print("=" * 80)

        # Train the system with adapted parameters
        results = self._train_system(
            regime_type=primary_regime,
            market_data=market_data,
            config=config,
            num_episodes=num_episodes,
            steps_per_episode=steps_per_episode,
            seed=seed
        )

        # Save results with regime information
        results['detected_regimes'] = {
            regime: len(periods) for regime, periods in detected_periods.items()
        }
        results['primary_regime'] = primary_regime

        self._save_results(results, primary_regime)

        return results

    def train_on_predefined_regimes(
        self,
        num_episodes: int = 500,
        steps_per_episode: int = 100,
        seed: int = 42
    ) -> Dict[str, Any]:
        """
        Train on all three predefined market regimes and compare.
        """
        print("\n" + "=" * 80)
        print("üéØ TRAINING ON ALL PREDEFINED MARKET REGIMES")
        print("=" * 80)

        regimes = {
            'high_risk': {
                'name': 'Ë∂ÖÂ§ßÈ£éÈô©Êúü (COVID-19 Crash)',
                'loader': lambda: self.data_loader.get_crisis_data('covid_2020'),
                'emoji': 'üî¥'
            },
            'high_return': {
                'name': 'Ë∂ÖÂ§ßÊî∂ÁõäÊúü (Post-COVID Rally)',
                'loader': lambda: self.data_loader.get_bull_market_data('post_covid_2020'),
                'emoji': 'üü¢'
            },
            'stable': {
                'name': 'Âπ≥Á®≥Êúü (Pre-COVID 2019)',
                'loader': lambda: self.data_loader.get_stable_market_data(2019),
                'emoji': 'üü°'
            }
        }

        all_results = {}

        for regime_key, regime_info in regimes.items():
            print(f"\n{regime_info['emoji']} {'=' * 76}")
            print(f"   REGIME: {regime_info['name']}")
            print("=" * 80)

            try:
                # Load data
                market_data = regime_info['loader']()

                # Get configuration
                config = self._get_regime_adaptive_config(regime_key, market_data)

                # Train
                results = self._train_system(
                    regime_type=regime_key,
                    market_data=market_data,
                    config=config,
                    num_episodes=num_episodes,
                    steps_per_episode=steps_per_episode,
                    seed=seed
                )

                all_results[regime_key] = results
                print(f"\n‚úÖ {regime_info['name']} completed")

            except Exception as e:
                print(f"\n‚ùå {regime_info['name']} failed: {e}")
                import traceback
                traceback.print_exc()
                all_results[regime_key] = {'error': str(e)}

        # Print comparison
        self._print_regime_comparison(all_results)

        # Save combined results (clean non-serializable objects first)
        output_file = self.output_dir / 'all_regimes_comparison.json'
        cleaned_results = self._clean_for_json(all_results)
        with open(output_file, 'w') as f:
            json.dump(cleaned_results, f, indent=2)

        print(f"\n‚úÖ Combined results saved: {output_file}")

        # ========== Generate Comprehensive Cross-Regime Evaluation ==========
        print(f"\n{'='*80}")
        print(f"üìä GENERATING CROSS-REGIME EVALUATION")
        print(f"{'='*80}")

        # Extract metrics for regime comparison
        regime_metrics_dict = {}
        for regime_key, results in all_results.items():
            if 'error' not in results and 'final_performance' in results:
                regime_metrics_dict[regime_key] = results['final_performance']

        if regime_metrics_dict:
            # Use evaluator to create regime comparison visualization
            from evaluation.performance_metrics import StrategyMetrics

            regime_metrics_objects = {}
            for regime_key, perf in regime_metrics_dict.items():
                # Create StrategyMetrics from performance dict
                regime_metrics_objects[regime_key] = perf

            self.evaluator.visualizer.plot_regime_comparison(
                regime_metrics=regime_metrics_objects,
                save_name="all_regimes_comparison.png"
            )

            print(f"\n‚úÖ Cross-regime evaluation completed!")

        return all_results

    def _load_data(self, source: str, symbols: list, start: str, end: str) -> Dict:
        """Load market data from specified source."""
        if source == "yahoo":
            if not symbols:
                symbols = ['^GSPC', 'AAPL', 'MSFT']
            return self.data_loader.load_multiple_assets(symbols, start, end, '1d')
        else:
            raise ValueError(f"Unsupported data source: {source}")

    def _determine_primary_regime(self, detected_periods: Dict) -> str:
        """Determine the primary market regime from detected periods."""
        # Count total days in each regime
        regime_days = {}
        for regime, periods in detected_periods.items():
            total_days = sum(p.end_idx - p.start_idx for p in periods)
            regime_days[regime] = total_days

        if not regime_days:
            return 'stable'  # Default

        # Return regime with most days
        return max(regime_days, key=regime_days.get)

    def _get_regime_adaptive_config(self, regime: str, data: Dict) -> Dict[str, Any]:
        """
        Get regime-adaptive configuration.

        Different regimes require different parameter settings.
        """
        # Base config
        config = {
            'latent_dim': 32,
            'num_factors': 10,
            'num_assets': len(data),
            'encoder_hidden_dim': 128,
            'hft_buffer_size': 100000,
            'mft_buffer_size': 50000,
            'lft_buffer_size': 50000,
        }

        # Encoder base config (from yaml file or defaults)
        encoder_config = {
            'num_layers': 4,
            'num_heads': 8,
            'hidden_dim': 256,
            'dropout': 0.1,
            'orthogonal_constraint': True,
            'num_lstm_layers': 2,
            'num_cnn_layers': 3,
            'transformer_weight': 0.6
        }

        # Regime-specific adaptations
        if regime == 'high_risk':
            # üî¥ Conservative settings for crisis periods
            # Use LSTM for fast response
            config.update({
                'lft_risk_aversion': 0.8,  # High risk aversion
                'hft_transaction_cost': 0.0003,  # Higher costs (wider spreads)
                'mft_correlation_penalty': 0.7,  # Higher correlation penalty
                'min_allocation_per_agent': 0.05,  # More diversification
                'allocator_lr': 2e-4,  # Slower adaptation
                'encoder_type': 'lstm',  # Fast response for crisis
                'encoder_config': encoder_config,
                'regime_description': 'Conservative - High risk period (LSTM encoder)'
            })

        elif regime == 'high_return':
            # üü¢ Aggressive settings for bull markets
            # Use Ensemble for robustness
            config.update({
                'lft_risk_aversion': 0.3,  # Low risk aversion
                'hft_transaction_cost': 0.0001,  # Lower costs (tight spreads)
                'mft_correlation_penalty': 0.3,  # Lower penalty
                'min_allocation_per_agent': 0.01,  # Less diversification
                'allocator_lr': 5e-4,  # Faster adaptation
                'encoder_type': 'ensemble',  # Best of both worlds
                'encoder_config': encoder_config,
                'regime_description': 'Aggressive - Bull market (Ensemble encoder)'
            })

        else:  # stable
            # üü° Balanced settings for normal markets
            # Use Transformer for deep analysis
            config.update({
                'lft_risk_aversion': 0.5,  # Moderate risk aversion
                'hft_transaction_cost': 0.0002,  # Normal costs
                'mft_correlation_penalty': 0.5,  # Moderate penalty
                'min_allocation_per_agent': 0.03,  # Moderate diversification
                'allocator_lr': 3e-4,  # Normal adaptation
                'encoder_type': 'transformer',  # Deep analysis for stable market
                'encoder_config': encoder_config,
                'regime_description': 'Balanced - Stable market (Transformer encoder)'
            })

        return config

    def _train_system(
        self,
        regime_type: str,
        market_data: Dict,
        config: Dict,
        num_episodes: int,
        steps_per_episode: int,
        seed: int
    ) -> Dict[str, Any]:
        """
        Train the complete multi-frequency system with CBR warm start.

        ÂÆåÊï¥ÊµÅÁ®ãÔºö
        STEP 1: ËÆ°ÁÆóÂ∏ÇÂú∫Êù°‰ª∂
        STEP 2: ‰ªéËÆ∞ÂøÜÂ∫ìÊ£ÄÁ¥¢warm startÁ≠ñÁï•
        STEP 3: ÂàùÂßãÂåñagentsÔºàÁî®warm startÂèÇÊï∞Ôºâ
        STEP 4: RLËÆ≠ÁªÉ
        STEP 5: Êõ¥Êñ∞ËÆ∞ÂøÜÂ∫ì
        """
        print(f"\nüìä STEP 1: Computing market conditions...")

        # Reset goal tracker at the start of each training cycle
        self.goal_tracker.reset()

        # Compute baseline statistics and market conditions
        main_symbol = list(market_data.keys())[0]
        df = market_data[main_symbol]
        prices = df['Close'].values
        returns = np.diff(prices) / prices[:-1]

        # Baseline performance
        baseline = {
            'total_return': float((prices[-1] / prices[0] - 1) * 100),
            'volatility': float(np.std(returns) * np.sqrt(252) * 100),
            'sharpe_ratio': float(np.mean(returns) / np.std(returns) * np.sqrt(252)) if np.std(returns) > 0 else 0.0,
            'num_periods': len(prices)
        }

        # Market conditions (for CBR similarity matching)
        market_conditions = {
            'volatility': baseline['volatility'] / 100,  # Normalize to [0,1]
            'trend': float(np.mean(returns)),  # Average return (positive = uptrend)
            'correlation': 0.5  # Default, would compute from multi-asset if available
        }

        print(f"\nüìà Market Conditions:")
        print(f"  Volatility: {market_conditions['volatility']:.4f}")
        print(f"  Trend: {market_conditions['trend']:.4f}")
        print(f"  Baseline Return: {baseline['total_return']:.2f}%")
        print(f"  Baseline Sharpe: {baseline['sharpe_ratio']:.2f}")

        # ========== Goal Planner Directive ==========
        planner_inputs = PlannerInputs(
            market_regime=regime_type,
            market_conditions=market_conditions,
            baseline_metrics=baseline,
            goal_snapshot=self.goal_tracker.snapshot()
        )
        directive = self.goal_planner.generate_directive(planner_inputs)

        print(f"\nüß≠ Goal Planner Directive:")
        for key, value in directive.to_dict().items():
            print(f"  {key}: {value}")

        # Incorporate directive into configuration
        config['goal_directive'] = directive.to_dict()
        config['goal_name'] = self.goal_definition.name
        config['goal_style_notes'] = directive.style_notes
        config['goal_risk_targets'] = directive.risk_targets
        config['goal_allocation_targets'] = directive.allocation_targets
        config['goal_constraint_overrides'] = directive.constraint_overrides

        config['min_allocation_per_agent'] = max(
            config.get('min_allocation_per_agent', directive.allocation_floor),
            directive.allocation_floor
        )
        if directive.latency_budget_ms is not None:
            config['latency_budget_ms'] = directive.latency_budget_ms
        if directive.capital_budget is not None:
            config['capital_budget'] = directive.capital_budget
        if directive.risk_targets.get('risk_aversion') is not None:
            config['lft_risk_aversion'] = directive.risk_targets['risk_aversion']

        # ========== STEP 1.5: Offline pretraining preparation ==========
        print("\nüß± OFFLINE PRETRAINING (CBR Aggregate)")
        pretrain_summary = self.offline_pretrainer.run(regime_type, market_conditions)
        for agent_type, summary in pretrain_summary.items():
            print(f"   {agent_type.upper()}: cases={len(summary.selected_cases)} avg Sharpe={summary.avg_sharpe:.2f}")

        # ========== STEP 2: Retrieve warm start strategies from memory ==========
        print(f"\nüîç STEP 2: Retrieving strategies from memory bank...")

        warm_starts = self.memory_bank.warm_start_all_agents(
            regime_type=regime_type,
            market_conditions=market_conditions
        )

        # ========== STEP 3: Initialize agents with warm start ==========
        print(f"\nüèóÔ∏è  STEP 3: Initializing agents...")

        agents_initialized = {
            'hft': warm_starts['hft'] is not None,
            'mft': warm_starts['mft'] is not None,
            'lft': warm_starts['lft'] is not None,
            'allocator': warm_starts['allocator'] is not None
        }

        print(f"  Configuration: {config.get('regime_description', 'Default')}")
        print(f"  Warm start status: {sum(agents_initialized.values())}/4 agents")

        # ========== STEP 3.5: Initialize regime-adaptive encoder ==========
        print(f"\nüß† STEP 3.5: Initializing shared encoder for {regime_type} regime...")

        # Get encoder info for logging
        encoder_info = get_encoder_info(regime_type)
        print(f"  üìä Encoder Type: {encoder_info['type']}")
        print(f"  ‚úÖ Advantages:")
        for adv in encoder_info['advantages']:
            print(f"     ‚Ä¢ {adv}")
        print(f"  ‚ö†Ô∏è  Trade-offs:")
        for trade in encoder_info['trade_offs']:
            print(f"     ‚Ä¢ {trade}")
        print(f"  üéØ Best for: {encoder_info['best_for']}")

        # Create encoder using factory
        encoder = create_encoder(
            regime_type=regime_type,
            latent_dim=config['latent_dim'],
            num_factors=config['num_factors'],
            config=config.get('encoder_config', {})
        )

        # Store encoder in config for downstream use
        config['encoder'] = encoder
        config['encoder_info'] = encoder_info

        # ========== STEP 4: RL Training ==========
        print(f"\nüöÄ STEP 4: RL Training ({num_episodes} episodes)...")

        # TODO: Actual agent training would happen here
        # For now, simulate training results

        # Simulate improved performance (in real system, this comes from training)
        simulated_improvement = 1.2 if any(agents_initialized.values()) else 1.0

        # Generate simulated returns for evaluation
        num_periods = len(prices)
        simulated_returns = np.random.normal(
            loc=np.mean(returns) * simulated_improvement,
            scale=np.std(returns) * 0.8,  # Lower volatility than baseline
            size=num_periods
        )

        # Ensure some positive drift
        simulated_returns += np.mean(returns) * simulated_improvement * 0.5

        # Calculate simulated capital
        initial_capital = 100000
        simulated_capital = initial_capital * np.cumprod(1 + simulated_returns)

        latency_budget = (
            directive.latency_budget_ms if directive.latency_budget_ms is not None else 25.0
        )
        capital_budget = (
            directive.capital_budget if directive.capital_budget is not None else initial_capital
        )
        capital_usage = capital_budget

        for r in simulated_returns:
            self.goal_tracker.update_step(
                realized_return=float(r),
                latency_ms=latency_budget,
                capital_in_use=capital_usage
            )

        goal_snapshot = self.goal_tracker.snapshot()

        print(f"\n{'='*80}")
        print(f"üìä STEP 4.5: COMPREHENSIVE STRATEGY EVALUATION")
        print(f"{'='*80}")

        # Evaluate strategy performance
        strategy_metrics = self.evaluator.evaluate_strategy(
            returns=simulated_returns,
            capital_series=simulated_capital,
            benchmark_returns=returns,
            regime=regime_type,
            agent_name='system',
            save_report=True
        )

        # Use evaluated metrics for final performance
        final_performance = {
            'sharpe_ratio': strategy_metrics.sharpe_ratio,
            'total_return': strategy_metrics.total_return,
            'max_drawdown': strategy_metrics.max_drawdown,
            'win_rate': strategy_metrics.win_rate,
            'sortino_ratio': strategy_metrics.sortino_ratio,
            'calmar_ratio': strategy_metrics.calmar_ratio,
            'volatility': strategy_metrics.volatility,
            'var_95': strategy_metrics.var_95,
            'cvar_95': strategy_metrics.cvar_95,
            'profit_factor': strategy_metrics.profit_factor
        }

        print(f"\nüéØ Goal Satisfaction Snapshot:")
        print(goal_snapshot.to_dict())

        report_path, _goal_payload = self.goal_audit.create_report(
            goal=self.goal_definition,
            snapshot=goal_snapshot,
            regime=regime_type,
            metadata={
                "primary_regime": regime_type,
                "goal_name": self.goal_definition.name
            }
        )
        print(f"\nüìù Goal audit saved to: {report_path}")

        goal_health = self.goal_tracker.health_snapshot()

        # ========== STEP 5: Update memory bank ==========
        print(f"\nüíæ STEP 5: Updating memory bank...")

        # Store strategies for each agent (mock parameters for now)
        for agent_type in ['hft', 'mft', 'lft', 'allocator']:
            # Mock strategy parameters (in real system, these would be network weights)
            strategy_params = {
                'network_weights': f'mock_weights_for_{agent_type}',
                'optimizer_state': f'mock_optimizer_state',
                'config': config,
                'warm_started': agents_initialized[agent_type],
                'goal_health': goal_health.to_dict(),
                'goal_directive': directive.to_dict(),
                'goal_snapshot': goal_snapshot.to_dict()
            }

            self.memory_bank.store_strategy(
                regime_type=regime_type,
                agent_type=agent_type,
                strategy_params=strategy_params,
                performance_metrics=final_performance,
                market_conditions=market_conditions,
                training_episodes=num_episodes
            )

        print(f"\n‚úÖ All agent strategies stored in memory bank")
        print(f"   These will be available for warm start in future runs!")

        # Compile results
        results = {
            'regime_type': regime_type,
            'baseline_stats': baseline,
            'final_performance': final_performance,
            'warm_start_used': agents_initialized,
            'market_conditions': market_conditions,
            'config': config,
            'offline_pretraining': {agent: summary.to_dict() for agent, summary in pretrain_summary.items()},
            'goal': self.goal_definition.to_dict(),
            'goal_directive': directive.to_dict(),
            'goal_satisfaction': goal_snapshot.to_dict(),
            'goal_health': goal_health.to_dict(),
            'goal_report_path': str(report_path),
            'training_params': {
                'num_episodes': num_episodes,
                'steps_per_episode': steps_per_episode,
                'seed': seed
            },
            'data_info': {
                'symbols': list(market_data.keys()),
                'num_periods': len(df),
                'start_date': str(df.index[0]),
                'end_date': str(df.index[-1])
            }
        }

        return results

    def _clean_for_json(self, obj):
        """
        Recursively clean objects to make them JSON-serializable.
        Removes or converts non-serializable objects like encoders, numpy arrays, etc.
        """
        if isinstance(obj, dict):
            cleaned = {}
            for key, value in obj.items():
                if key in ['encoder', 'encoder_info']:
                    # Replace encoder with a string representation
                    if key == 'encoder':
                        cleaned[key] = '<Encoder object - not serializable>'
                    elif isinstance(value, dict):
                        # Keep encoder_info if it's a dict (it should be serializable)
                        cleaned[key] = self._clean_for_json(value)
                    else:
                        cleaned[key] = str(value)
                else:
                    cleaned[key] = self._clean_for_json(value)
            return cleaned
        elif isinstance(obj, list):
            return [self._clean_for_json(item) for item in obj]
        elif isinstance(obj, (np.ndarray, np.generic)):
            return obj.tolist()
        elif isinstance(obj, GoalDefinition):
            return obj.to_dict()
        elif hasattr(obj, "to_dict"):
            try:
                return obj.to_dict()
            except TypeError:
                return str(obj)
        elif hasattr(obj, '__dict__') and not isinstance(obj, (str, int, float, bool, type(None))):
            # Complex object that's not JSON-serializable
            return str(obj)
        else:
            return obj

    def _save_results(self, results: Dict, regime: str):
        """Save training results."""
        output_file = self.output_dir / f'{regime}_results.json'
        cleaned_results = self._clean_for_json(results)
        with open(output_file, 'w') as f:
            json.dump(cleaned_results, f, indent=2)

        print(f"\nüíæ Results saved: {output_file}")

    def _print_regime_comparison(self, results: Dict[str, Any]):
        """Print comparison across regimes."""
        print("\n" + "=" * 80)
        print("üìä REGIME COMPARISON SUMMARY")
        print("=" * 80)

        print(f"\n{'Regime':<20} {'Return%':>12} {'Vol%':>12} {'Sharpe':>12} {'Periods':>12}")
        print("-" * 80)

        for regime in ['high_risk', 'high_return', 'stable']:
            if regime in results and 'baseline_stats' in results[regime]:
                stats = results[regime]['baseline_stats']
                print(f"{regime:<20} {stats['total_return']:>12.2f} "
                      f"{stats['volatility']:>12.2f} {stats['sharpe_ratio']:>12.2f} "
                      f"{stats['num_periods']:>12}")

        print("=" * 80)


def main():
    parser = argparse.ArgumentParser(
        description="Regime-Adaptive Multi-Frequency Trading System"
    )

    parser.add_argument('--mode', type=str, default='auto',
                        choices=['auto', 'all_regimes', 'single'],
                        help='Training mode')

    parser.add_argument('--regime', type=str,
                        choices=['high_risk', 'high_return', 'stable'],
                        help='Specific regime (for single mode)')

    parser.add_argument('--data-source', type=str, default='yahoo',
                        choices=['yahoo', 'csv', 'predefined'],
                        help='Data source')

    parser.add_argument('--symbols', type=str, nargs='+',
                        default=['^GSPC', 'AAPL', 'MSFT'],
                        help='Trading symbols')

    parser.add_argument('--start', type=str, default='2020-01-01',
                        help='Start date (YYYY-MM-DD)')

    parser.add_argument('--end', type=str, default='2021-12-31',
                        help='End date (YYYY-MM-DD)')

    parser.add_argument('--episodes', type=int, default=500,
                        help='Number of training episodes')

    parser.add_argument('--steps', type=int, default=100,
                        help='Steps per episode')

    parser.add_argument('--seed', type=int, default=42,
                        help='Random seed')

    parser.add_argument('--goal-config', type=str,
                        help='Path to personalized goal configuration (YAML/JSON)')

    parser.add_argument('--gpu', action='store_true',
                        help='Use GPU if available')

    parser.add_argument('--output', type=str, default='outputs',
                        help='Output directory')

    args = parser.parse_args()

    goal_parser = GoalParser()
    goal_definition = goal_parser.load(args.goal_config)
    print(f"\nLoaded goal definition: {goal_definition.name}")

    # Initialize system
    system = RegimeAdaptiveTrainingSystem(
        goal_definition=goal_definition,
        output_dir=args.output,
        verbose=True
    )

    # Run based on mode
    if args.mode == 'all_regimes' or args.data_source == 'predefined':
        # Train on all three predefined regimes
        results = system.train_on_predefined_regimes(
            num_episodes=args.episodes,
            steps_per_episode=args.steps,
            seed=args.seed
        )

    elif args.mode == 'auto':
        # Auto-detect regime and train
        results = system.train_with_regime_detection(
            data_source=args.data_source,
            symbols=args.symbols,
            start_date=args.start,
            end_date=args.end,
            num_episodes=args.episodes,
            steps_per_episode=args.steps,
            seed=args.seed
        )

    print("\n" + "=" * 80)
    print("‚úÖ TRAINING COMPLETED")
    print("=" * 80)
    print(f"\nResults saved in: {args.output}/")
    print("\nNext steps:")
    print("  1. Review results: cat outputs/*_results.json")
    print("  2. Run on different regimes: python train.py --mode all_regimes")
    print("  3. Custom data: python train.py --symbols BTC-USD --start 2021-01-01")
    print("=" * 80)


if __name__ == "__main__":
    main()
