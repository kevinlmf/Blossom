"""
EM Training Loop for Learning Latent Variables that Explain Returns

å®Œæ•´çš„EMè®­ç»ƒå¾ªç¯ï¼š
1. E-step: ä½¿ç”¨å½“å‰encoderæå–latent factors z_t
2. M-step: æœ€å¤§åŒ–z_tå¯¹æ”¶ç›ŠR_tçš„è§£é‡Šèƒ½åŠ›ï¼Œæ›´æ–°encoder
3. è¯„ä¼°: è®¡ç®—RÂ²ç­‰æŒ‡æ ‡
4. å¾ªç¯ç›´åˆ°æ€§èƒ½è¶…è¿‡baseline
"""

import jax
import jax.numpy as jnp
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
from pathlib import Path
import json
from tqdm import tqdm

from .em_encoder import (
    create_em_encoder,
    e_step,
    m_step,
    compute_r_squared
)
from evaluation import PerformanceMetrics, BenchmarkStrategies


class EMReturnLearning:
    """
    EMç®—æ³•å­¦ä¹ èƒ½è§£é‡Šèµ„äº§æ”¶ç›Šçš„latent variables
    
    ç›®æ ‡ï¼šæ‰¾åˆ°z_tä½¿å¾— R_t = f(z_t) + Îµï¼Œæœ€å¤§åŒ–RÂ²
    """
    
    def __init__(
        self,
        market_data: np.ndarray,
        returns: np.ndarray,
        latent_dim: int = 64,
        num_factors: int = 10,
        learning_rate: float = 1e-3,
        output_dir: str = "outputs/em_return_learning",
        max_iterations: int = 50,
        min_r_squared: float = 0.3,
        verbose: bool = True
    ):
        """
        åˆå§‹åŒ–EMå­¦ä¹ å™¨
        
        Args:
            market_data: å¸‚åœºæ•°æ® [T, seq_len, input_dim] æˆ– [T, input_dim]
            returns: èµ„äº§æ”¶ç›Š [T, num_assets]
            latent_dim: æ½œåœ¨å˜é‡ç»´åº¦
            num_factors: å› å­æ•°é‡
            learning_rate: å­¦ä¹ ç‡
            output_dir: è¾“å‡ºç›®å½•
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            min_r_squared: æœ€å°RÂ²é˜ˆå€¼
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        """
        self.market_data = market_data
        self.returns = returns
        self.latent_dim = latent_dim
        self.num_factors = num_factors
        self.learning_rate = learning_rate
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.max_iterations = max_iterations
        self.min_r_squared = min_r_squared
        self.verbose = verbose
        
        # å‡†å¤‡æ•°æ®
        self._prepare_data()
        
        # åˆ›å»ºencoderå’Œè®­ç»ƒçŠ¶æ€
        self.encoder, self.state = create_em_encoder(
            latent_dim=latent_dim,
            num_factors=num_factors,
            num_assets=returns.shape[1] if len(returns.shape) > 1 else 1,
            learning_rate=learning_rate
        )
        
        self.key = jax.random.PRNGKey(42)
        
        # å†å²è®°å½•
        self.r_squared_history = []
        self.loss_history = []
        self.latent_factors_history = []
        
        if verbose:
            print("\n" + "="*80)
            print("ğŸ”„ EM ALGORITHM FOR LEARNING LATENT VARIABLES")
            print("="*80)
            print(f"Market Data Shape: {market_data.shape}")
            print(f"Returns Shape: {returns.shape}")
            print(f"Latent Dimension: {latent_dim}")
            print(f"Number of Factors: {num_factors}")
            print(f"Max Iterations: {max_iterations}")
            print(f"Min RÂ² Threshold: {min_r_squared}")
            print("="*80)
    
    def _prepare_data(self):
        """å‡†å¤‡æ•°æ®æ ¼å¼"""
        # å¦‚æœmarket_dataæ˜¯2Dï¼Œè½¬æ¢ä¸º3D [T, seq_len, features]
        if len(self.market_data.shape) == 2:
            T, features = self.market_data.shape
            seq_len = 20  # é»˜è®¤åºåˆ—é•¿åº¦
            
            # åˆ›å»ºæ»‘åŠ¨çª—å£
            market_data_3d = []
            for t in range(T):
                start_idx = max(0, t - seq_len + 1)
                seq_data = self.market_data[start_idx:t+1]
                
                # Padding if needed
                if len(seq_data) < seq_len:
                    padding = np.zeros((seq_len - len(seq_data), features))
                    seq_data = np.concatenate([padding, seq_data], axis=0)
                
                market_data_3d.append(seq_data)
            
            self.market_data = np.array(market_data_3d)  # [T, seq_len, features]
        
        # ç¡®ä¿returnsæ˜¯2D
        if len(self.returns.shape) == 1:
            self.returns = self.returns[:, None]  # [T, 1]
    
    def compute_baseline_r_squared(self) -> float:
        """
        è®¡ç®—baselineçš„RÂ²ï¼ˆä½¿ç”¨ç®€å•ç‰¹å¾ï¼‰
        
        Returns:
            baseline_r_squared: Baselineçš„RÂ²
        """
        # ä½¿ç”¨å¸‚åœºæ•°æ®çš„ç®€å•ç‰¹å¾ä½œä¸ºbaseline
        # ä¾‹å¦‚ï¼šä½¿ç”¨ä»·æ ¼å˜åŒ–ç‡
        if len(self.market_data.shape) == 3:
            # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„ç‰¹å¾
            simple_features = self.market_data[:, -1, :]  # [T, features]
        else:
            simple_features = self.market_data
        
        # ä½¿ç”¨å‰å‡ ä¸ªç‰¹å¾é¢„æµ‹æ”¶ç›Š
        if simple_features.shape[1] > 0:
            # ç®€å•çš„çº¿æ€§å›å½’
            F = simple_features[:, :min(5, simple_features.shape[1])]  # ä½¿ç”¨å‰5ä¸ªç‰¹å¾
            R = self.returns
            
            # æ·»åŠ æˆªè·
            F_with_intercept = np.concatenate([
                np.ones((F.shape[0], 1)), F
            ], axis=1)
            
            # OLSå›å½’
            try:
                betas = np.linalg.lstsq(F_with_intercept, R, rcond=None)[0]
                R_pred = F_with_intercept @ betas
                
                ss_res = np.sum((R - R_pred) ** 2, axis=0)
                ss_tot = np.sum((R - np.mean(R, axis=0)) ** 2, axis=0)
                r_squared = 1 - ss_res / (ss_tot + 1e-8)
                
                baseline_r_squared = float(np.mean(r_squared))
            except:
                baseline_r_squared = 0.0
        else:
            baseline_r_squared = 0.0
        
        if self.verbose:
            print(f"\nğŸ“Š Baseline RÂ²: {baseline_r_squared:.4f}")
        
        return baseline_r_squared
    
    def run(self) -> Dict[str, Any]:
        """
        è¿è¡ŒEMè®­ç»ƒå¾ªç¯
        
        Returns:
            æœ€ç»ˆç»“æœå­—å…¸
        """
        # è®¡ç®—baseline
        baseline_r_squared = self.compute_baseline_r_squared()
        
        best_r_squared = -np.inf
        best_iteration = 0
        
        if self.verbose:
            print(f"\nğŸš€ Starting EM Training Loop...")
            print(f"Target: RÂ² > {max(baseline_r_squared, self.min_r_squared):.4f}")
        
        for iteration in range(self.max_iterations):
            if self.verbose:
                print(f"\n{'='*80}")
                print(f"ğŸ”„ EM Iteration {iteration + 1}/{self.max_iterations}")
                print(f"{'='*80}")
            
            # E-step: ä¼°è®¡æ½œåœ¨å˜é‡
            if self.verbose:
                print("ğŸ“Š E-step: Estimating latent variables...")
            
            latent_factors = e_step(
                self.encoder,
                self.state.params,
                jnp.array(self.market_data),
                self.key
            )
            
            if self.verbose:
                print(f"  Extracted latent factors: {latent_factors.shape}")
                print(f"  Factor statistics:")
                print(f"    Mean: {jnp.mean(latent_factors):.4f}")
                print(f"    Std: {jnp.std(latent_factors):.4f}")
            
            # M-step: æ›´æ–°encoderå‚æ•°
            if self.verbose:
                print("ğŸ”„ M-step: Updating encoder to maximize return explanation...")
            
            self.state, metrics, self.key = m_step(
                self.encoder,
                self.state,
                jnp.array(self.market_data),
                jnp.array(self.returns),
                latent_factors,
                self.key,
                num_steps=10
            )
            
            # è®¡ç®—RÂ²
            r_squared = compute_r_squared(
                latent_factors,
                jnp.array(self.returns)
            )
            
            # è®°å½•å†å²
            self.r_squared_history.append(float(r_squared))
            self.loss_history.append(float(metrics['total_loss']))
            self.latent_factors_history.append(np.array(latent_factors))
            
            if self.verbose:
                print(f"\nğŸ“Š Performance Metrics:")
                print(f"  RÂ² (Return Explanation): {r_squared:.4f}")
                print(f"  Baseline RÂ²: {baseline_r_squared:.4f}")
                print(f"  Improvement: {r_squared - baseline_r_squared:+.4f}")
                print(f"  Total Loss: {metrics['total_loss']:.6f}")
                print(f"  MSE Loss: {metrics['mse_loss']:.6f}")
                print(f"  RÂ² Loss: {metrics['r2_loss']:.6f}")
            
            # æ›´æ–°æœ€ä½³æ€§èƒ½
            if r_squared > best_r_squared:
                best_r_squared = r_squared
                best_iteration = iteration + 1
            
            # æ£€æŸ¥æ”¶æ•›æ¡ä»¶
            target_r_squared = max(baseline_r_squared, self.min_r_squared)
            
            if r_squared >= target_r_squared:
                if self.verbose:
                    print(f"\nâœ… CONVERGED! RÂ² exceeds target.")
                    print(f"  RÂ²: {r_squared:.4f} >= Target: {target_r_squared:.4f}")
                    print(f"  Iterations: {iteration + 1}")
                
                result = {
                    'converged': True,
                    'iteration': iteration + 1,
                    'final_r_squared': float(r_squared),
                    'baseline_r_squared': baseline_r_squared,
                    'improvement': float(r_squared - baseline_r_squared),
                    'best_r_squared': float(best_r_squared),
                    'best_iteration': best_iteration,
                    'latent_factors': np.array(latent_factors),
                    'r_squared_history': self.r_squared_history,
                    'loss_history': self.loss_history
                }
                
                self._save_results(result, converged=True)
                return result
            
            # æ£€æŸ¥æ˜¯å¦ä¸å†æå‡
            if iteration > 5:
                recent_r_squared = self.r_squared_history[-5:]
                if max(recent_r_squared) - min(recent_r_squared) < 0.01:
                    if self.verbose:
                        print(f"\nâš ï¸  RÂ² converged (no improvement in last 5 iterations)")
        
        # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
        if self.verbose:
            print(f"\nâš ï¸  Reached maximum iterations ({self.max_iterations})")
            print(f"  Best RÂ²: {best_r_squared:.4f} at iteration {best_iteration}")
            print(f"  Baseline RÂ²: {baseline_r_squared:.4f}")
        
        result = {
            'converged': False,
            'iteration': self.max_iterations,
            'final_r_squared': float(self.r_squared_history[-1]),
            'baseline_r_squared': baseline_r_squared,
            'improvement': float(self.r_squared_history[-1] - baseline_r_squared),
            'best_r_squared': float(best_r_squared),
            'best_iteration': best_iteration,
            'latent_factors': np.array(self.latent_factors_history[-1]),
            'r_squared_history': self.r_squared_history,
            'loss_history': self.loss_history
        }
        
        self._save_results(result, converged=False)
        return result
    
    def _save_results(self, result: Dict[str, Any], converged: bool):
        """ä¿å­˜ç»“æœ"""
        # ä¿å­˜JSONç»“æœ
        result_file = self.output_dir / f"em_results_iter_{result['iteration']}.json"
        
        # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨
        result_to_save = {}
        for k, v in result.items():
            if isinstance(v, np.ndarray):
                result_to_save[k] = v.tolist()
            else:
                result_to_save[k] = v
        
        with open(result_file, 'w') as f:
            json.dump(result_to_save, f, indent=2)
        
        if self.verbose:
            print(f"\nğŸ’¾ Results saved to: {result_file}")
    
    def get_latent_factors(self) -> np.ndarray:
        """è·å–æœ€ç»ˆçš„latent factors"""
        if self.latent_factors_history:
            return self.latent_factors_history[-1]
        else:
            # å¦‚æœæ²¡æœ‰å†å²ï¼Œé‡æ–°è®¡ç®—
            latent_factors = e_step(
                self.encoder,
                self.state.params,
                jnp.array(self.market_data),
                self.key
            )
            return np.array(latent_factors)









